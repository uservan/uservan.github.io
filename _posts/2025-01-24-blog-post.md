---
title: 'Thinking Preference Optimization'
date: 2025-01-24
permalink: /posts/2025/01/Thinking_Preference_Optimization/
tags:
  - LLM
---

# Thinking Preference Optimization

This approach demonstrates a practical and efficient way to improve a model’s ability to perform complex reasoning tasks. By leveraging a curated combination of fast-thinking and slow-thinking data with DPO training, gains in performance can be achieved, even with a modest dataset size.

## Overview

Improving a model’s slow thinking capability has become an essential step in model development. The quickest and most convenient approach is to collect extensive high quality datasets designed for slow thinking and fine-tune the model through supervised fine-tuning (SFT). Beyond this, we found that utilizing a small dataset specifically curated for **both fast and slow thinking**—such as 5,000 examples—can enhance the model’s reasoning capabilities. 

In this blog, we introduce our method, share the datasets and hyperparameters used, and demonstrate the impact of our approach.

## Method

### Data Curation
To implement this method, two types of datasets are required:
1. **SFT Data**:
    - [OpenO1-SFT](https://huggingface.co/datasets/VanWang/OpenO1-SFT-Pro-Filter): 
        - Based on `O1-OPEN/OpenO1-SFT` dataset, we remove special tokens (e.g., <Thought\>) and reformat final answers into the $\box$ format using GPT-4o-mini. we also filter out non-English entries and sequences longer than 8,192 tokens. The final dataset size is approximately 80,000 examples.
    - NuminaMath-CoT:
        - Randomly select examples from `AI-MO/NuminaMath-CoT` to match the size of the OpenO1-SFT dataset, used for comparison.
    - [Sky-T1_data_17k](https://huggingface.co/datasets/VanWang/SKY-SFT): 
        - Based on `NovaSky-AI/Sky-T1_data_17k`, we remove special characters and reformat the final answer into QWQ's output style.
    
2. **[DPO Data](https://huggingface.co/datasets/VanWang/NuminaMath-CoT_O1_Qwq)**:
    - **Rejected Answers**: A random selection of 5,000 examples from `AI-MO/NuminaMath-CoT` , where the dataset’s original answers were treated as the “rejected” entries.
    - **Chosen Answers**: New answers for each problem were generated using `Qwen/QwQ-32B-Preview` model.
    - **Filtering**: Only entries where both the “chosen” and “rejected” answers were correct were included, ensuring high-quality data.

### Training
1.	SFT Training:
	- Each SFT dataset is used to train a `LLaMA-3.1-8B` model for one epoch with a learning rate of 3e-5 and a batch size of 32.
2.	DPO Training:
	- Using the 5,000 curated examples, we perform DPO training on the SFT-trained models.
	- As a baseline, we also conduct DPO training directly on `LLaMA 3.1 8B-Instruct` model.
	- All DPO training is with the beta of 0.01 and a batch size of 32.
    

## Results

- We present the results of models trained with different datasets using SFT, followed by DPO training, on the test dataset.

### Performance on AIME, MATH500, and MATH Overall 


| **SFT Dataset/Model Family** | **Variant**                 | **AIME** | **MATH500** | **MATH (Overall)** |
| ---------------------------- | --------------------------- | -------- | ----------- | ------------------ |
| **LLaMA 8B-Instruct**        | **base model**              | 0.02     | 0.288       | 0.33               |
|                              | dpo(lr=3e-7) | 0.04     | 0.286       | 0.34               |
|  **NuminaMath-CoT**  | sft(lr=3e-5) | 0.0      | 0.162       | 0.25               |
|            | dpo(lr=1e-7) | 0.0      | 0.176       | 0.26            |
|  **OpenO1-SFT**              | sft(lr=3e-5)            | 0.0      | 0.23        | 0.33               |
|                              | dpo(lr=1e-7) | 0.01     | 0.234       | 0.35               |
|                              | **dpo(lr=3e-7)** | **0.01**     | **0.284**      | **0.40**               |
|  **Sky-T1_data_17k**      | sft(lr=3e-5 )            | 0.01     | 0.236       | 0.33               |
|         | dpo(lr=1e-7) | 0.0      | 0.252       | 0.36                                     |

    
    
### Performance on MATH across different levels 
- To further observe the model’s performance in mathematical reasoning, we collect 100 data points for each level in MATH dataset, and test the model’s scores on datasets from different levels.


| **SFT Dataset/Model Family** | **Variant**              | **MATH1** | **MATH2** | **MATH3** | **MATH4** | **MATH5** |
| ---------------------------- | ------------------------ | --------- | --------- | --------- | --------- | --------- |
| **LLaMA 8B-Instruct**        | **base model**           | 0.59      | 0.39      | 0.37      | 0.20      | 0.10      |
|                              | dpo(lr=3e-7) | 0.66      | 0.33      | 0.36      | 0.25      | 0.13      |
|   **NuminaMath-CoT**    | sft(lr=3e-5 )         | 0.45      | 0.31      | 0.19      | 0.09      | 0.05      |
|                         | dpo(lr=1e-7) | 0.51      | 0.27      | 0.24      | 0.11      | 0.01      |
|  **OpenO1-SFT**       | sft(lr=3e-5)         | 0.58      | 0.40      | 0.31      | 0.20      | 0.10      |
|                       | dpo(lr=1e-7) | 0.67      | 0.38      | 0.33      | 0.20      | 0.09      |
|                              | **dpo(lr=3e-7)** | **0.73**      | **0.45**      | **0.40**      | **0.28**      | **0.09**      |
|  **Sky-T1_data_17k**   | sft(lr=3e-5)      | 0.61      | 0.36      | 0.33      | 0.22      | 0.08      |
|                       | dpo(lr=1e-7) | 0.68      | 0.40      | 0.26      | 0.22      | 0.14      |


## Thoughts and Future Work
- A simple DPO approach can enhance the model’s mathematical reasoning ability.
- Deepseek R1 can be used to further improve the dataset quality, thereby further boosting the model’s mathematical reasoning performance.
- From the results, it appears that the model’s improvement is primarily seen in datasets with lower difficulty, which is related to the inherent difficulty of the dataset. In the future, different difficulty levels of datasets can be tried to observe the model’s performance.